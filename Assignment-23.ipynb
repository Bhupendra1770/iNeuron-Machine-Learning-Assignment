{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256120c6",
   "metadata": {},
   "source": [
    "### 1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8975f174",
   "metadata": {},
   "source": [
    "Ans: Disadvantages of Dimensionality Reduction: \n",
    "- It may lead to some amount of data loss.\n",
    "- PCA fails in cases where mean and covariance are not enough to define datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae85551",
   "metadata": {},
   "source": [
    "### 2. What is the dimensionality curse ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113d77b",
   "metadata": {},
   "source": [
    "Ans: The curse of dimensionality basically means that the error increases with the increase in the number of features. It refers to the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981a756",
   "metadata": {},
   "source": [
    "### 3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22cd70e",
   "metadata": {},
   "source": [
    "Ans: No, dimensionality reduction is not reversible in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b2a64",
   "metadata": {},
   "source": [
    "### 4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77953e0",
   "metadata": {},
   "source": [
    "Ans: PCA is a linear dimensionality reduction technique which is good at dealing with linear datasets, but when it comes to nonlinear datasets, it may not be effective. Instead, non-linear dimensionality reduction techniques are more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b772162e",
   "metadata": {},
   "source": [
    "### 5. Assume you're running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a75145",
   "metadata": {},
   "source": [
    "Ans: If I perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. In this case roughly 950 dimensions are required to preserve 95% of the variance. So the answer is, it depends on the dataset, and it could be any number between 1 and 950."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114163d1",
   "metadata": {},
   "source": [
    "### 6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a77b3",
   "metadata": {},
   "source": [
    "Ans:The following are the scenarios where the following are used: \n",
    "- Vanilla PCA: the dataset fit in memory\n",
    "- Incremental PCA: larget dataset that don't fit in memory, online taks\n",
    "- Randomized PCA: considerably reduce dimensionality and the dataset fit the memory.\n",
    "- kernel PCA: used for nonlinear PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b649e",
   "metadata": {},
   "source": [
    "### 7. How do you assess a dimensionality reduction algorithm's success on your dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ec18f",
   "metadata": {},
   "source": [
    "Ans: By doing PCA, it is a good choice for dimensionality reduction and visualization for datasets with a large number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17620cf8",
   "metadata": {},
   "source": [
    "### 8. Is it logical to use two different dimensionality reduction algorithms in a chain ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6f153",
   "metadata": {},
   "source": [
    "Ans: Yes, it is logical to use multiple dimensionality reduction techniques in a chain, also known as \"ensemble dimensionality reduction\". It is used to combine the advantages of different algorithms and to overcome their limitations. The order of the algorithms used in the chain can also be important, as the output of one algorithm can be used as input for the next one.\n",
    "\n",
    "For example, you could use PCA first to reduce the dimensionality of a dataset and then use t-SNE to further reduce the dimensionality while preserving non-linear structure of the data. This way, you could take advantage of the linear structure captured by PCA while also preserving non-linear structure of the data captured by t-SNE.\n",
    "\n",
    "However, it's important to note that this approach may increase the complexity of the model and the interpretability of the results may decrease. It is important to consider the trade-off between model complexity and interpretability when using multiple dimensionality reduction techniques in a chain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
